# Architecture Decision Records (Draft Bundle)

> **Note:** In the repository, split each ADR below into its own file named `ADR-XXX-<slug>.md` and keep them in the `docs/adr/` folder. Crossâ€‘link them from the README under an *Architecture Decisions* section. Follow the numbering sequence strictlyâ€”once published, the file numbers must never change.

---

## ADRâ€‘001 â€“Â Adopt Architecture Decision Records (ADR) Methodology

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

We need a lightweight, versionâ€‘controlled way to capture the â€œwhyâ€ behind key technical choices so that future maintainers (internal or external) can understand and evolve the system without guessâ€‘work.

### Decision

- Adopt the Markdownâ€‘based *Architecture Decision Record* format popularised by MichaelÂ Nygard (see [template](https://github.com/joelparkerhenderson/architecture-decision-record)).
- Store each ADR in `docs/adr/ADRâ€‘NNNâ€‘<slug>.md` with incremental numbering.
- Use the sections **Context / Decision / Consequences / References**.
- Status keywords: *Proposed*, *Accepted*, *Superseded*, *Deprecated*, *Rejected*.

### Consequences

- Project history stays discoverable via Git.
- New contributors can align with existing decisions instead of reâ€‘litigating them.

### References

- MichaelÂ Nygard, *Lightweight Architecture Decision Records* (2011)
- joelparkerhenderson/architectureâ€‘decisionâ€‘record GitHub repo

---

## ADRâ€‘002 â€“Â Dataâ€‘Lake Layering Strategy (RawÂ â†’Â TransformÂ â†’Â Stage)

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

The client explicitly requires a threeâ€‘layer storage pattern, and we want to align with the industryâ€‘standard Medallion/Lakehouse approach (Bronze/Silver/Gold) for clarity and scalability.

### Decision

- Map layers as follows:
  - **Raw (Bronze)** â€“Â untouched outputs from source APIs stored as JSON (AQS) or CSV (Envista). Immutable, appendâ€‘only.
  - **Transform (Silver)** â€“Â cleaned, standardised, unitâ€‘converted data stored as columnar Parquet files partitioned by pollutant and year.
  - **Stage (Gold)** â€“Â Powerâ€‘BIâ€‘optimised tidy CSVs plus helper tables (monitorÂ coverage, AQIÂ daily) partitioned by pollutant and year.
- Folder naming: `<layer>/<source>/<pollutant>/<YYYY>/â€¦`.

### Consequences

- Clear contract between steps enables independent unitâ€‘testing and parallelism.
- Stage files stay small enough (<Â 250Â MB) for fast Power BI refresh.

### References

- Databricks blog, *What is a Medallion Architecture?* (2022)

---

## ADRâ€‘003 â€“Â Language Choice: Python 3.12 for ETL Pipelines

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

The legacy code is in R but the client wants a Pythonâ€‘centric, modular, easilyâ€‘deployable solution on Windows.

### Decision

- Rewrite all ETL logic in PythonÂ 3.12 using only widelyâ€‘supported libraries (requests, pandas, pyarrow, geopandas, pythonâ€‘dotenv).
- Provide a `requirements.txt` to pin exact versions.
- Keep the original R scripts in `reference/` for traceability; they are not executed.

### Consequences

- Easier hiring and maintenance (larger Python talent pool).
- Higher performance for I/Oâ€‘bound tasks due to async options (future).

---

## ADRâ€‘004 â€“Â File Formats & Partitioning

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

PowerÂ BI handles CSV well for final consumption, but intermediate transforms benefit from columnar formats.

### Decision

- **Raw**: keep original format (JSON or CSV) to preserve fidelity.
- **Transform**: store as Snappyâ€‘compressed Parquet.
- **Stage**: write UTFâ€‘8 CSV with ISOâ€‘8601 timestamps.
- Partition Prq/CSV by `pollutant/year` to cap folder size and enable pruning.

### Consequences

- 8Ã— faster load for Transform layer; Stage remains instantly queryable by Powerâ€¯BIâ€™s folder connector.

---

## ADRâ€‘005 â€“Â Authoritative Source & Fallback Logic

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

EPAÂ AQS is the gold standard but releases certified data with delay. Envista provides nearâ€‘realtime hourly values plus extra metadata.

### Decision

- For each hour, prefer AQS sample values when available; otherwise pull Envista.
- Keep *both* sources in Raw for auditability.
- In Transform, when deduplicating, persist the chosen source in a `data_source` column.

### Consequences

- Continuous data stream even during AQS gaps; provenance remains transparent.

---

## ADRâ€‘006 â€“Â Envista Qualifier â†’ Simplified Qual Mapping

**Status**: *Proposed â€“Â 2025â€‘05â€‘25*

### Context

Envista qualifier codes are verbose and sourceâ€‘specific; PowerÂ BI users need a compact set similar to AQS qualifiers (e.g., `V` = *valid*, `I` = *invalid*, `M` = *missing*).

### Decision

| Envista Code | Meaning                | Simple Qual |
| ------------ | ---------------------- | ----------- |
| 0            | Valid                  | V           |
| 803          | Instrument calibration | I           |
| 998          | Missing data           | M           |
| â€¦            | â€¦                      | â€¦           |

The mapping YAML will live in `config/envista_quality_map.yml` and be applied during Transform.

### Consequences

- Unified semantics across AQS & Envista streams.
- Future Envista code additions require only a YAML update, not code change.

---

## ADRâ€‘007 â€“Â Orchestration: Windows Task Scheduler

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

The clientâ€™s deployment target is a Windows workstation with no container runtime.

### Decision

- Provide a signed `register_task.ps1` that imports a `.xml` task definition (daily 02:00 local).
- The task runs `python main.py --schedule daily` in a dedicated service account with minimal privileges.

### Consequences

- Zero external dependencies; standard Windows tooling.
- Must monitor exits codes via email alert script; TaskÂ Schedulerâ€™s own logging is not sufficient.

---

## ADRâ€‘008 â€“Â Logging & Monitoring

**Status**: *Accepted â€“Â 2025â€‘05â€‘25*

### Context

Silent failures are a top risk for unattended ETL.

### Decision

- Adopt the [`loguru`](https://github.com/Delgan/loguru) library as the projectâ€‘standard logger instead of the stdlibÂ `logging` module because it offers easier configuration, structured JSON output and builtâ€‘in rotation.
- Configure two sinks at application startâ€‘up:
  - **File sink**Â `logs/pipeline.log` with `rotation="10 MB"` and `retention=7`, `compression="zip"`.
  - **Stderr sink** for interactive runs, with minimum levelÂ `WARNING`.
- Enable `serialize=True` on both sinks so each line is emitted as a JSON object: `{"time": "...", "level": "...", "name": "...", "message": "..."}`.
- All library modules call `from loguru import logger` and log with `logger.info/debug/warning(...)`; no child loggers or manual handlers are required.

### Consequences

- Configuration is reduced to \~5 lines and can live in `scripts/run_pipeline.py`.
- JSON lines are immediately parseable by PowerÂ BI or any SIEM without custom formatters.
- Builtâ€‘in rotation plus compression prevents uncontrolled log growth without external utilities.

---

## README Crossâ€‘Links (snippet)

```markdown
## ğŸ—‚ Architecture Decisions
This project uses [ADR](https://adr.github.io/) to capture major technical choices:

|Â ID | Purpose |
|----|---------|
| ADRâ€‘001 | Adopt ADR methodology |
| ADRâ€‘002 | Dataâ€‘Lake Layering Strategy |
| ADRâ€‘003 | Language Choice: Python |
| ADRâ€‘004 | File Formats & Partitioning |
| ADRâ€‘005 | Authoritative Source & Fallback Logic |
| ADRâ€‘006 | Envista Qualifier Mapping |
| ADRâ€‘007 | Orchestration: Windows Task Scheduler |
| ADRâ€‘008 | Logging & Monitoring |
```

---

> **Next step:** move each ADR to its own file, mark ADRâ€‘006 as *Accepted* once the client approves the exact qualifier mapping list.

